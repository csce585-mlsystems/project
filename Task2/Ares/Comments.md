# Task 2 --- Team-Ares

Thank you Team-Ares for you work!

Feedback:
1. Good
    1. Great breakdown.
    2. Reasoning the selection of weak defenses and data for the experiment.
    3. Very clear experiment setting.
    4. Good metric of training and validation.
2. Other comments
    1. Improper data were used to train and test your model. Training and testing the model on the same AE variants will lead to the overfitting on such samples. When building a defense, we are facing an open-ended problem, in which the attacker can generate an AE using any attack with any possible adversarial configurations. It will be very expensive to presume so many AE variants during training. You did not feel the pain because we provided the AEs for you. If you generate these AEs by yourself and during the training, you might quite due to the computational cost (time and space). Besides, it can result in a model that overfit on the attacks used during training and is still vulnerable against other attacks. So, a more reasonable experiment setting is to train the model on (1) the clean data (a.k.a. legitimate data, benign samples) or (2) the clean data plus data generated by an attack optimizer (refer to ``PGD-ADT``). Ideally, you train the model using the clean training data (``60k`` in total) from the dataset (Frameworks like ``Tensorflow``, ``PyTorch``, and ``Keras`` provide APIs to download the clean training/test data automatically). Consider the time issue, in this task, you can split the clean test data (in total ``10k``) in to training and validation data (typically ``0.8:0.2``).      
    2. Did not evaluate your model. ATHENA is a framework for building adversarial defense. So, all tasks based on ATHENA require evaluating some models on (some) AEs. Check the ``README.md`` for ``Task2`` for the general processing for this option. The detailed information regarding the model itself would be less concerned in this task, unless you are seeking insights like how the model type/architecture/hyperparameters impact model's robustness.
    3. Cannot find ``tensorflow tutorial`` in your submissions/repo (Detailed steps -> Create model). If you meant the official tensorflow tutorial, make ``tensorflow tutorial`` as a link or cite the tutorial at the end of the report would be a good idea.
    4. The title for the ``Validation and Training Metrics`` might be wrong? Do you mean the strategy model rather than ensemble?
    5. Missing the reference/citation section.
