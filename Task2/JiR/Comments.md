# Task 2 --- Team-JiR

Thank you Team-JiR for you work!

Feedback
1. Good
    1. The introduction session is great!
    2. Citations.
2. To improve
    1. List the paths (locations) to all relevant files to this task, such as the json files for configurations, the python files (or Jupyter notebooks) for scripts (implemented or modified by you), files for results, location for generated AEs, etc.
    2. **Experiment**
        1. Report 
            * The training data is not clear. Specify the training data for each of your models if they are not the same --- ``"The two ML models were trained either on the predictions of the weak defenses or on the AEs generated in Task 1." (Testing session)`` (1) If the model were trained on the predictions of the weak defense, from what data (the ``clean data`` or ``AEs``) you collected the predictions? When you mention ``predictions`` of weak defenses, do you mean the ``probabilities``, ``logits``, or ``outputs from some middle layers``? (2) If the models were trained on the ``AEs``, it conflicted with what you claimed in the **Training** session (``"We obtain an array of dimensions (x, y, 10), where x is the number of weak defenses, ..."``) and you cannot get an input shape you expected (``(num_of_WDs, num_of_samples, 10)``).
            * If you collected predictions of weak defenses from AEs for training the strategy models, then improper data were used to train and test your models. Training and testing the strategy model on the same AE variants will lead to the overfitting on such samples. When building a defense, we are facing an open-ended problem, in which the attacker can generate an AE using any attack with any possible adversarial configurations. It will be very expensive to presume so many AE variants during training. You did not feel the pain because we provided the AEs for you. If you generate these AEs by yourself and during the training, you might quite due to the computational cost (time and space). Besides, it can result in a model that overfit on the attacks used during training and is still vulnerable against other attacks. So, a more reasonable experiment setting is to train the model on (1) the clean data (a.k.a. legitimate data, benign samples) or (2) the clean data plus data generated by an attack optimizer (refer to ``PGD-ADT``). Ideally, you train the model using the clean training data (``60k`` in total) from the dataset (Frameworks like ``Tensorflow``, ``PyTorch``, and ``Keras`` provide APIs to download the clean training/test data automatically). Consider the time issue, in this task, you can split the clean test data (in total ``10k``) in to training and validation data (typically ``0.8:0.2``).
            * How many strategy models were built and trained in this task? In the **Testing** session, it said ``2`` ML models were trained. However, in the **Evaluation of defenses** session, results for ``3`` models (using ``3`` different activation functions ---``ReLU``, ``ELU``, and ``sigmoid``) were reported. You can name your models when you first mention them, then refer to the model names in the rest of the report, which will make the description easier and clearer.  
            * The evaluation is not clear. (1) What does ``Model against itself`` stands for? Evaluation on the training data (I presume that ``Versus benign samples`` refers to the evaluation on the clean test data)? (2) On what AEs variant (attack and parameters) the evaluation of ``Versus Task 1 AEs`` was performed? Mixed set? If so, then this evaluation does not provide any insights, the important information of the AEs were lost. The parameters of an attack somehow indicates the strength of the generated AEs, e.g., with a larger ![\Large \epsilon](https://latex.codecogs.com/svg.latex?\Large\epsilon), ``FGSM`` generates stronger AEs.  Moreover, the effectiveness of a defense may vary among different attacks. A defense that is effective against one attack (or the mixed AEs) does not infer that the effectiveness against another attack.
        2. Scripts (implement)
            * The strategy model was incorrectly implemented. The input-shape of the strategy model is ``(28, 28)`` (``ourScripts.task2_modelBuilding_ik.trainNewModel``, line ``26``), and the strategy model is trained and tested directly on images instead of predictions from weak defenses (``ourScripts.task2_modelBuilding_ik.trainNewModel``, lines ``52-66``). So, in this task, you actually trained new **classifiers** (instead of strategy model, alternative to the fixed strategy ``MV``, ``AVEP``, etc.) on the clean test data. You collected and saved raw predictions from weak defenses (``ourScripts/task2_getEnsemblePredictions``, lines ``86``, ``91``, ``105``, and ``108``) but never used them anywhere.
            * Did not find the evaluation on AEs in your script. Only found the evaluations on the benign samples (``ourScripts.task2_modelBuilding_ik.trainNewModel``, lines ``61-66``).
    3. Suggest to plot the learning curves into a single figure.
    4. ATHENA is a framework for building adversarial defense. So, all tasks based on ATHENA require evaluating some models on (some) AEs. Check the ``README.md`` for ``Task2`` for the general processing for this option. The detailed information regarding the model itself would be less concerned in this task, unless you are seeking insights like how the model type/architecture/hyperparameters impact model's robustness. 
    5. No contributions of individual members.   
